# -*- coding: utf-8 -*-
"""Splitting dataset into weekday and weekend.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DiCjRtliBM-Qge1J8Xiw_Lpxm45SGAND
"""

# Commented out IPython magic to ensure Python compatibility.
# Spliting dataset to improve accuracy
# 1) Use KNN model to predict the passenger flow on weekday, since the passenger flow on weekday is more stable
# 2) Use LSTM model to predict the passenger flow on weekend, since LST mdoel can predict extreme value
# 3) Calculate accuracy of KNN, accuracy of LSTM and the accuracy of spliting datasets
# 4) Plot the predict value and actual value

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.metrics import mean_absolute_error,mean_squared_error
from sklearn.preprocessing import MinMaxScaler
import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsRegressor
import matplotlib.pyplot as plt
# % matplotlib inline
import warnings 
warnings.filterwarnings('ignore')
from google.colab import drive
order = ['Day','wdsp', 'rain','dayofmonth','dayofweek','dayofyear','Easter','Christmas', 'Summer/winter_holiday','Patrick','Total']
data_lough = data_lough[order]
# split dataset according to weekday "1.0,2.0,3.0,4.0,5.0"
weekday = data_lough.loc[data_lough['dayofweek'].isin([1.0,2.0,3.0,4.0,5.0])]
weekend = data_lough.loc[data_lough['dayofweek'].isin([6.0,7.0])]
data=weekend.iloc[:,0:13].values 
lstm_dict = pd.DataFrame({'Acctural_value': data_lough['Total'][0:990]})
#hidden layer units
hidden_layer_unit=6   
# insert 10 features
input_size= 10  
# outpi
output_size=1
# learning rate is 0.01
lr=0.001     
tf.reset_default_graph()
#Input layer, output layer weight, offset
weights={
         'in':tf.Variable(tf.random_normal([input_size,hidden_layer_unit])),
         'out':tf.Variable(tf.random_normal([hidden_layer_unit,1]))
         }
biases={
        'in':tf.Variable(tf.constant(0.1,shape=[hidden_layer_unit,])),
        'out':tf.Variable(tf.constant(0.1,shape=[1,]))
        }


#  build lstm model 
def lstm(X):  
    batch_size=tf.shape(X)[0]
    time_step=tf.shape(X)[1]
    w_in=weights['in']
    b_in=biases['in']  
    #Need to convert tensor into 2 dimensions for calculation, and the calculated result is used as the input of the hidden layer.
    input=tf.reshape(X,[-1,input_size])  
    input_rnn=tf.matmul(input,w_in)+b_in
    # Convert tensor to 3D as input to lstm cell
    input_rnn=tf.reshape(input_rnn,[-1,time_step,hidden_layer_unit])  
    cell=tf.contrib.rnn.BasicLSTMCell(hidden_layer_unit)
    #cell=tf.contrib.rnn.core_rnn_cell.BasicLSTMCell(rnn_unit)
    init_state=cell.zero_state(batch_size,dtype=tf.float32)
    #output_rnn is the result of recording each output node of lstm, final_states is the result of the last cell
    output_rnn,final_states=tf.nn.dynamic_rnn(cell, input_rnn,initial_state=init_state, dtype=tf.float32)
    #Input as input layer
    output=tf.reshape(output_rnn,[-1,hidden_layer_unit]) 
    w_out=weights['out']
    b_out=biases['out']
    pred=tf.matmul(output,w_out)+b_out
    return pred,final_states
  
def get_data(batch_size=60,time_step=20,train_begin=0,train_end=1598):
    batch_index_list=[]
#     scale data between 0 and 1
    scaler_x=MinMaxScaler(feature_range=(0,1))  
    scaler_y=MinMaxScaler(feature_range=(0,1))
    dd = np.array(data[:,-1]).reshape(284  ,1)
    data_scaled_x=scaler_x.fit_transform(data[:,:-1])
    data_scaled_y=scaler_y.fit_transform(dd)
    train_data_normalized, test_data_normalized, train_label, test_label = train_test_split(data_scaled_x, data_scaled_y, test_size=0.2,random_state = 0)

#     get train x a and train y
    x_train_data,y_train_data=[],[]   
    for i in range(len(train_data_normalized)-time_step):
        if i % batch_size==0:
            batch_index_list.append(i)
        x=train_data_normalized[i:i+time_step,:10]
        y=train_label[i:i+time_step,np.newaxis]
        
        x_train_data.append(x.tolist())
        y_train_data.append(y.tolist())
    batch_index_list.append((len(train_data_normalized)-time_step))
    
    
    size=(len(test_data_normalized)+time_step-1)//time_step   
    x_test_data,y_test_data=[],[]  
    for i in range(size-1):
        x=test_data_normalized[i*time_step:(i+1)*time_step,:10]
        y=test_label[i*time_step:(i+1)*time_step]
        x_test_data.append(x.tolist())
        y_test_data.extend(y)
    x_test_data.append((test_data_normalized[(i+1)*time_step:,:10]).tolist())
    y_test_data.extend((test_label[(i+1)*time_step:]).tolist()) 
    return batch_index_list,x_train_data,y_train_data,x_test_data,y_test_data,scaler_y
 
#  calculate the accuracy, if the actual value smaller than predict value then use actual value / predict value
#  if the predict value is smaller than actual value then use predict value / actual value
#  finally, calculate the average of the accuracy
def tt_accuracy(predict,test):
    total = 0
    for i in range(len(predict)):
        if abs(test[i]) <= abs(predict[i]):
           a = abs(test[i]) / abs(predict[i])
        else:
            a =  abs(predict[i]) / abs(test[i]) 
        total += a
    accuracy = total / len(predict)
    return accuracy
  
#  train LSTM model 
def train_lstm(batch_size=80,time_step=15,train_begin=0,train_end=1598):
    X=tf.placeholder(tf.float32, shape=[None,time_step,input_size])
    Y=tf.placeholder(tf.float32, shape=[None,time_step,output_size])
    batch_index_list,x_train_data,y_train_data,test_x,test_y,scaler_y = get_data(batch_size,time_step,train_begin,train_end)
    pred,_=lstm(X)
    #loss function
    loss=tf.reduce_mean(tf.square(tf.reshape(pred,[-1])-tf.reshape(Y, [-1])))
    train_op=tf.train.AdamOptimizer(lr).minimize(loss) 
    saver=tf.train.Saver(tf.global_variables())
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        #iterator time is 500
        iter_time = 500
        for i in range(iter_time):
            for step in range(len(batch_index_list)-1):
                feed_dict={X:np.reshape(x_train_data[batch_index_list[step]:batch_index_list[step+1]], (-1,15,10)),Y:np.reshape(y_train_data[batch_index_list[step]:batch_index_list[step+1]], (-1,15,1))}
                _,loss_=sess.run([train_op,loss],feed_dict)
        test_predict=[]       
        for step in range(len(test_x)-1): 
            prob=sess.run(pred,feed_dict={X:np.reshape([test_x[step]],(-1,15,10))})   
            predict=prob.reshape((-1))
            test_predict.extend(predict)
#         inverse tansfor predicted value, since it was transformed between 0 and 1,
#         we need to inverse transform it to original range so that can calculate accuarcy
        test_predict = scaler_y.inverse_transform(np.reshape(test_predict,(-1,1)))
        test_y = scaler_y.inverse_transform(test_y)
        rmse=np.sqrt(mean_squared_error(test_predict,test_y[0:45]))
#       calculate accuracy by using "tt_accuracy" function
        real = tt_accuracy(test_predict,test_y[0:45])
#         calculate mean squared error
        mean_squared = mean_squared_error(test_y[0:45],test_predict)
        mean_error = np.sqrt(mean_squared_error(test_y[0:45],test_predict))
#     calculate mean absolute error
        mae = mean_absolute_error(y_pred=test_predict,y_true=test_y[0:45])
        print ('mae:',mae,'   rmse:',rmse)
        print('Athenry Road: LSTM accuracy is :',real)
    return test_predict,real,test_y[0:45]
test_predict,real,test_y = train_lstm(batch_size=80,time_step=15,train_begin=0,train_end=1598)


#  calculate the accuracy, if the actual value smaller than predict value then use actual value / predict value
#  if the predict value is smaller than actual value then use predict value / actual value
#  finally, calculate the average of the accuracy
def accuracy_knn(predict,test):
    total = 0
    for i in range(len(predict)):
        if abs(test[i]) <= abs(predict[i]):
           a = abs(test[i]) / abs(predict[i])
        else:
            a =  abs(predict[i]) / abs(test[i]) 
        total += a
    accuracy = total / len(predict)
    return accuracy        
 
n_dots = 1140
label = weekday["Total"].astype(np.float64)
data = weekday.drop('Total',1)
X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.2,random_state = 0)

#add noise to y
y_train += 0.1 * np.random.rand(n_dots) - 0.1
 
#KNN Regression, set k is 3 which means avergae 3 cloest neigbours' value
k = 3
knn = KNeighborsRegressor(k)
knn.fit(X_train,y_train)
#Calculate the fitting accuracy of the fitted curve for training samples
prec = knn.score(X_train, y_train)  
# making prediction
y_pred = knn.predict(X_test)
# calculate accuracy by using "accuracy_knn" function
r = accuracy_knn(y_pred,y_test.tolist())
print('the accuracy of Athenry road in KNN model is :',0.2*real+0.8*r)
print('the accuracy of spliting dataset is :',r)
# plot predict value and actual value, visualize weekday and weekend separately
axia = [i for i in range(1,287)]
plt.plot(axia,y_test.tolist(), c='yellow', label='weekday actual value')
plt.plot(axia,y_pred, c='red',label='weekday predict value')
weekend = [i for i in range(288,333)]
plt.plot(weekend,test_y, c='yellow', label='weekend actual value')
plt.plot(weekend,test_predict, c='green',label='weekend predict value')
plt.xlabel('data')
plt.ylabel('passenger flow')
plt.title('the performace of KNN and LSTM on both weekday and weekend')
plt.legend(loc = 3)
plt.figure()